{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download documents from U.S. Census Bureau to local directory.\n",
    "os.makedirs(\"us_census\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2022/demo/p70-178.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-017.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-016.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-015.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"us_census\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load pdf files in the local directory\n",
    "loader = PyPDFDirectoryLoader(\"./us_census/\")\n",
    "\n",
    "docs_before_split = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "docs_after_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')Before split, there were 63 documents loaded, with average characters equal to 3830.After split, there were 296 documents (chunks), with average characters equal to 864 (average chunk length)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings with hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\", \n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"What were the trends in median household income across\n",
    "           different states in the United States between 2021 and 2022.\"\"\"  \n",
    "         # Sample question, change to other questions you are interested in.\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Hub\n",
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "To use, we should have the huggingface_hub python package installed.\n",
    "Set an environment variable called HUGGINGFACEHUB_API_TOKEN with your Hugging Face access token in it.\n",
    "\n",
    "Currently, HuggingFace LangChain integration doesn’t support the question-answering task, so we can’t select HuggingFace QA models for this project. Instead, we select LLMs from the text-generation task category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "\n",
    "query = \"\"\"What were the trends in median household income across different states in the United States between 2021 and 2022.\"\"\"  # Sample question, change to other questions you are interested in.\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RetrievalQA invoke method to execute the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
